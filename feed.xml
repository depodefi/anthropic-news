<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Anthropic News</title>
    <link>https://www.anthropic.com/news</link>
    <description>Latest news from Anthropic</description>
    <language>en-US</language>
    <lastBuildDate>Sun, 25 Jan 2026 08:04:10 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Claude's new constitution</title>
      <link>https://www.anthropic.com/news/claude-new-constitution</link>
      <description>A new approach to a foundational document that expresses and shapes who Claude is</description>
      <author>Anthropic</author>
      <pubDate>Thu, 22 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/claude-new-constitution</guid>
    </item>
    <item>
      <title>Mariano-Florentino Cuéllar appointed to Anthropic’s Long-Term Benefit Trust</title>
      <link>https://www.anthropic.com/news/mariano-florentino-long-term-benefit-trust</link>
      <description>Anthropic’s Long-Term Benefit Trust announced the appointment of Mariano-Florentino (Tino) Cuéllar as a new member of the Trust. The Long-Term Benefit Trust is an independent body designed to help Anthropic achieve its public benefit mission.  Cuéllar brings extensive experience in law, governance, and international affairs, including service as a Justice of the Supreme Court of California, leadership of Stanford's Freeman Spogli Institute for International Studies, and his current role as President of the Carnegie Endowment for International Peace. Cuéllarannounced plansto step down from Carnegie in July 2026, when he will return to Stanford University to lead the Center for Advanced Study in the Behavioral Sciences and the Knight-Hennessy Scholars Program.  He has served three U.S. presidential administrations and currently chairs the board of the William &amp; Flora Hewlett Foundation. Cuéllar offers a global perspective shaped by his upbringing along the U.S.-Mexico border and a career spanning immigration, criminal justice, public health, and regulatory reform. His work has consistently focused on how technology affects public institutions and democratic governance—including co-leading California's Working Group on AI Frontier Models alongside Fei-Fei Li, and serving on the National Academy of Sciences Committee on Social and Ethical Implications of Computing Research.  Neil Buddy Shah, Chair of the Long-Term Benefit Trust, said: &quot;As AI becomes a defining factor in geopolitical competition—reshaping economies, security, and the balance of power between nations—the Trust needs leaders who understand these dynamics. Tino's exceptional background in law, governance, and international affairs will be invaluable as we help Anthropic navigate a world where AI adoption by governments and institutions is accelerating rapidly.&quot;  Anthropic is a Public Benefit Corporation with a mission of ensuring a safe transition through transformative AI. The Long-Term Benefit Trust helps Anthropic achieve this public benefit mission by selecting members of Anthropic’s Board of Directors, and advising the Board and leadership on how the company can maximize the benefits of advanced AI and mitigate its risks. New Trustees are selected by existing Trustees, in consultation with Anthropic, and have no financial stake in Anthropic. The Trust’s composition reflects a recognition that transformative AI will affect more than technology or business, with significant implications for global health, international security and society as a whole.  Tino Cuéllar said: “As AI capabilities advance at an unprecedented pace, the need for governance structures that marry private sector dynamism with civic responsibility has never been more urgent. Anthropic’s leadership has demonstrated a genuine commitment to thinking deeply about the societal implications of their work—not just the technology, but its impact on global security, democratic institutions, and human welfare. The Long-Term Benefit Trust represents a thoughtful approach to ensuring that as these powerful systems evolve, decisions about their development remain grounded in the broader public interest. I’m honored to contribute my experience to this important work.”  The Trust also announced that Kanika Bahl and Zachary Robinson have concluded their terms as Trustees. Both joined the Trust at its founding and contributed significantly to establishing its role in Anthropic’s governance.  Daniela Amodei, President of Anthropic, said: &quot;I'm delighted to welcome Tino to the Trust. What I find most compelling about him is his ability to work across sectors—law, government, academia, and technology. As AI systems become more capable, we need leaders who have spent their careers thinking deeply about technology's role in society. We're also extremely grateful to Kanika Bahl and Zach Robinson for their contributions during the Trust's formative period—it would not be where it is today without them.&quot;  Buddy Shah, Chair of the Long-Term Benefit Trust, said: “I've been grateful for Kanika and Zach's partnership since the Trust was established in 2023. They helped build the LTBT from the ground up—including the work of appointing board members like Jay Kreps and Reed Hastings who have strengthened Anthropic's governance. I'm grateful for their service and proud of what we built together.&quot;  Kanika Bahl, CEO &amp; President of Evidence Action, said: “Serving on the Long-Term Benefit Trust has been a privilege during a pivotal time for both Anthropic and the broader AI field. I’ve been impressed by the seriousness with which Anthropic’s leadership approaches questions of safety and societal benefit. I wish the Trust and the entire Anthropic team continued success.”  Zachary Robinson, CEO of the Centre for Effective Altruism, said: “I've been honored by the opportunity to serve on the Long-Term Benefit Trust during these formative years. I continue to admire the commitment Anthropic's leadership and Trustees make to safety and public benefit, and I have been impressed by the degree to which that commitment has endured as Anthropic has scaled and evolved. I am grateful to know Anthropic remains in the hands of leaders who care deeply about its mission, and I wish them all success.”</description>
      <author>Anthropic</author>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/mariano-florentino-long-term-benefit-trust</guid>
    </item>
    <item>
      <title>Anthropic and Teach For All launch global AI training initiative for educators</title>
      <link>https://www.anthropic.com/news/anthropic-teach-for-all</link>
      <description>Anthropic is partnering with Teach For All to bring AI tools and training to educators in 63 countries. Through the AI Literacy &amp; Creator Collective (LCC), more than 100,000 teachers and alumni across Teach For All's network—which serves more than 1.5 million students—will have the opportunity to develop AI fluency and adapt Claude to serve real classroom needs. Building on the approach pioneered by Teach For America, Teach For All is a global network of independent organizations working to expand educational opportunity. Over the past 15 years, it has grown into one of the world's largest and most respected communities of educators serving students in under-resourced schools. Its network spans organizations like Teach For India, Enseña Chile, and Teach For Nigeria—each locally led but connected through a common mission and cross-network collaboration. What makes this partnership distinctive is its approach. Teachers are positioned not as passive consumers of AI tools, but as co-architects shaping how AI develops. Through the AI LCC, Anthropic provides access to Claude, while educators provide on-the-ground feedback to inform how the product evolves. “For AI to reach its potential to make education more equitable, teachers need to be the ones shaping how it's used and providing input on how it's designed,” said CEO of Teach For All Wendy Kopp. “Our partnership with Anthropic is helping educators across our network experiment with and learn from these tools firsthand, as co-creators of AI's role in education.” &quot;The combination of real-world experience from the Teach For All network and technical insights from Anthropic has provided a fabulous learning opportunity,&quot; said Michael Gilmore, COO of Teach for Australia. &quot;We look forward to continuing participation in 2026.&quot; One teacher in Liberia, new to AI, attended the AI LCC's live trainings on AI fluency. Within weeks, he had built aninteractive climate education curriculumfor Liberian schools using Claude Artifacts: interactive tools like apps, games, or visualizations that Claude can build on the spot. In Bangladesh, a teacher working with Grade 6 and 7 students—over half of whom struggled with basic numeracy—built agamified math learning appcomplete with boss battles, a leaderboard, and XP rewards. The pattern is consistent across the network: teachers who know their students best can now build tools tailored to them. &quot;After working with a few different AI tools, discovering Claude through the community initiative significantly expanded my practice,&quot; said Rosina Bastidas, a tech educator at Enseña por Argentina. &quot;I've since developed multiple educational artifacts and I'm currently designing digital, interactive workspaces for secondary school students aligned with the curriculum.&quot; The impact extends beyond individual classrooms to leadership as well. &quot;The partnership has connected us with a community of organizations navigating similar technical opportunities, and there's been significant learning around responsible AI implementation,&quot; said Oscar Onuoha, IT Lead at Teach For Nigeria. &quot;We're grateful to Anthropic for their commitment to supporting non-profits as we explore these emerging technologies.&quot; The collective operates through three interconnected programs. The AI Fluency Learning Series, developed with Anthropic's education team, consists of six live episodes covering AI fluency, Claude capabilities, and practical classroom applications. Over 530 educators attended the first series in November 2025. Claude Connect is the community's ongoing learning hub—more than 1,000 educators representing 60+ countries exchanging prompts, use cases, and discoveries through daily peer-to-peer conversation. Claude Lab is for educators interested in going further. This innovation space gives teachers Claude Pro access to test practical implementations with advanced features. Participants have monthly office hours with the Anthropic team and the opportunity to directly inform Claude's product roadmap. Within four days of announcing the program, we received over 200 applications. This partnership builds on our growing work with educators and governments worldwide. In Iceland, we launched one of the world's first comprehensive national AI education pilots. In Rwanda, we partnered with the government and ALX to bring AI education to hundreds of thousands of learners across Africa. And through initiatives like our participation in the White House Taskforce on AI Education, we're working to ensure students and educators across America develop practical AI skills. As AI transforms how knowledge is created and shared, teachers will be essential guides for students navigating this transition. The educators in this partnership are already showing what's possible: a climate curriculum built in Liberia, a math game designed in Bangladesh, and digital workspaces taking shape in Argentina. This is our commitment—ensuring that educators in every community, not just the most well-resourced, can shape and benefit from AI's potential. For more on Anthropic's education initiatives,see here. </description>
      <author>Anthropic</author>
      <pubDate>Wed, 21 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/anthropic-teach-for-all</guid>
    </item>
    <item>
      <title>Anthropic appoints Irina Ghose as Managing Director of India ahead of Bengaluru office opening</title>
      <link>https://www.anthropic.com/news/anthropic-appoints-irina-ghose-as-managing-director-of-india</link>
      <description>Irina Ghose is joining Anthropic as Managing Director of India as we prepare to open our first office in the country. Irina brings more than three decades of experience in scaling technology businesses. She most recently served as Managing Director, Microsoft India, where she led enterprise AI adoption across major Indian industries including banking and financial services, healthcare, manufacturing, and government. She’s led high-impact teams, built ecosystem partnerships, and championed future-ready capabilities across India’s technology landscape, with a consistent focus on using technology to drive meaningful business and societal impact. “India has a real opportunity to shape how AI is built and deployed at scale,” Irina said. “Indian organizations are moving beyond experimentation toward applied AI, where trust, safety, and long-term impact matter as much as innovation. Anthropic’s mission resonates with my belief that technology should empower people, expand access, and create lasting value across India’s diverse languages and communities.” &quot;Irina's expertise in scaling technology businesses and driving enterprise transformation makes her the ideal leader as we expand,&quot; said Chris Ciauri, Managing Director of International, Anthropic. &quot;As we grow our teams and deepen engagement across India’s public and private sectors, Irina will ensure our approach is grounded in local insight and aligned with our mission.&quot; Our India team will work closely with policymakers and academic institutions, strengthen developer engagement, and build partnerships with enterprises and organizations using AI to address local challenges. India ranks as the second-largest market globally for Claude.ai. Anthropic's fourthEconomic Indexshowed that Indian users have a striking focus on technical applications, with nearly half of all Claude.ai usage concentrated in computer and mathematical tasks.</description>
      <author>Anthropic</author>
      <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/anthropic-appoints-irina-ghose-as-managing-director-of-india</guid>
    </item>
    <item>
      <title>How scientists are using Claude to accelerate research and discovery</title>
      <link>https://www.anthropic.com/news/accelerating-scientific-research</link>
      <description>Last October we launched Claude for Life Sciences—a suite of connectors and skills that made Claude a better scientific collaborator. Since then,we've invested heavily in making Claude the most capable model for scientific work, with Opus 4.5 showing significant improvements in figure interpretation, computational biology, and protein understanding benchmarks. These advances, informed by our partnerships with researchers in academia and industry, reflect our commitment to understanding exactly how scientists are using AI to accelerate progress.  We’ve also been working closely with scientists through ourAI for Scienceprogram, which provides free API credits to leading researchers working on high-impact scientific projects around the world.  These researchers have developed custom systems that use Claude in ways that go far beyond tasks like literature reviews or coding assistance. In the labs we spoke to, Claude is a collaborator that works across all stages of the research process: making it easier and more cost-effective to understand which experiments to run, using a variety of tools to help compress projects that normally take months into hours, and finding patterns in massive datasets that humans might overlook. In many cases it’s eliminating bottlenecks, handling tasks that require deep knowledge and have previously been impossible to scale; in some it’s enabling entirely different research approaches than researchers have traditionally been able to take. In other words, Claude is beginning to reshape how these scientists work—and point them towards novel scientific insights and discoveries.  One bottleneck in biological research is the fragmentation of tools: there are hundreds of databases, software packages, and protocols available, and researchers spend substantial time selecting from and mastering various platforms. That’s time that, in a perfect world, would be spent on running experiments, interpreting data, or pursuing new projects. Biomni, an agentic AI platform from Stanford University, collects hundreds of tools, packages, and data-sets into a single system through which a Claude-powered agent can navigate. Researchers give it requests in plain English; Biomni automatically selects the appropriate resources. It can form hypotheses, design experimental protocols, and perform analyses across more than 25 biological subfields. Consider the example of a genome-wide association study (GWAS), a search for genetic variants linked to some trait or disease. Perfect pitch, for instance, has a strong genetic basis. Researchers would take a very large group of people—some who are able to produce a musical note without any reference tone, and others you would never invite to karaoke—and scan their genomes for genetic variants that show up more often in one group than another. The genome scanning is (relatively) simple. It’s the process of analyzing and making sense of the data that’s time-consuming: genomic data comes in messy formats and needs extensive cleaning; researchers must control for confounding and deal with missing data; once they identify any “hits,” they need to figure out what they actually mean—what gene is nearby (since GWAS only points to locations in a genome), what cell types it’s expressed in, what biological pathway it might affect, and so on. Each step might involve different tools, different file formats, and a lot of manual decision-making. It’s a tedious process. A single GWAS can take months. But in an early trial of Biomni, it took 20 minutes. This might sound too good to be true—can we be sure of the accuracy of this kind of AI analysis? The Biomni team hasvalidatedthe system through several case studies in different fields. In one, Biomni designed a molecular cloning experiment; in a blind evaluation, the protocol and design matched that of a postdoc with more than five years of experience. In another, Biomni analyzed the data from over 450 wearable data files from 30 different people (a mix of continuous glucose monitoring, temperature, and physical activity) in just 35 minutes—a task estimated to take a human expert three weeks. In a third, Biomni analyzed gene activity data from over 336,000 individual cells taken from human embryonic tissue. The system confirmed regulatory relationships scientists already knew about, but also identified new transcription factors—proteins that control when genes turn on and off—that researchers hadn’t previously connected to human embryonic development. Biomni isn’t a perfect system, which is why it includes guardrails to detect if Claude has gone off-track. Nor can it yet do everything out of the box. However, where it comes up short, experts can encode their methodology as askill—teaching the agent how an expert might approach a problem, rather than letting it improvise. For example, when working with the Undiagnosed Diseases Network on rare disease diagnosis, the team found that Claude's default approach differed substantially from what a clinician would do. So they interviewed an expert, documented their diagnostic process step by step, and taught it to Claude. With that new, previously-tacit knowledge, the agent performed well. Biomni represents one approach: a general-purpose system that brings hundreds of tools under one roof. But other labs are building more specialized systems—targeting specific bottlenecks in their own research workflows. When scientists want to understand what a gene does, one approach is to remove it from the cell or organism in question and see what breaks. The gene-editing tool CRISPR, which emerged around 2012, made this easy to do precisely at scale. But the utility of CRISPR was still limited: labs could generate far more data than they had the bandwidth to analyze. This is exactly the challenge faced by Iain Cheeseman’slabat the Whitehead Institute and Department of Biology at MIT. Using CRISPR, they knock out thousands of different genes across tens of millions of human cells, then photograph each cell to see what changed. The patterns in those images reveal that genes that do similar jobs tend to produce similar-looking damage when removed. Software can detect these patterns and group genes together automatically—Cheeseman's lab built a pipeline to do exactly this calledBrieflow(yes, brie the cheese). But interpreting what these gene groupings mean—why the genes cluster together, what they might have in common, whether it’s a known biological relationship or something new—still requires a human expert to comb through the scientific literature, gene by gene. It’s slow. A single screen can produce hundreds of clusters, and most never get investigated simply because labs don’t have the time, bandwidth, or in-depth knowledge about the diverse things that cells do. For years, Cheeseman did all the interpretation himself. He estimates he can recall the function of about 5,000 genes off the top of his head, but it still takes hundreds of hours to analyze this data effectively. To accelerate this process, PhD student Matteo Di Bernardo sought to build a system that would automate Cheeseman’s approach. Working closely with Cheeseman to understand exactly how he approaches interpretation—what data sources he consults, what patterns he looks for, what makes a finding interesting—they built a Claude-powered system calledMozzareLLM(you might be seeing a theme developing here). It takes a cluster of genes and does what an expert like Cheeseman would do: identifies what biological process they might share, flags which genes are well-understood versus poorly studied, and highlights which ones might be worth following up on. Not only does this substantially accelerate their work, but it is also helping them make important additional biological discoveries. Cheeseman finds Claude consistently catches things he missed. “Every time I go through I’m like, I didn’t notice that one! And in each case, these are discoveries that we can understand and verify,” he says. What helps make MozzareLLM so useful is that it isn’t a one-trick pony: it can incorporate diverse information and reason like a scientist. Most notably, it provides confidence levels in its findings, which Cheeseman emphasizes is crucial. It helps him decide whether or not to invest more resources in following up on its conclusions. In building MozzareLLM, Di Bernardo tested multiple AI models. Claude outperformed the alternatives—in one case correctly identifying an RNA modification pathway that other models dismissed as random noise. Cheeseman and Di Bernardo envision making these Claude-annotated datasets public—letting experts in other fields follow up on clusters his lab doesn't have time to pursue. A mitochondrial biologist, for instance, could dive into mitochondrial clusters that Cheeseman's team has flagged but never investigated. As other labs adopt MozzareLLM for their own CRISPR experiments, it could accelerate the interpretation and validation of genes whose functions have remained uncharacterized for years. The Cheeseman lab uses optical pooled screening—a technique that lets them knock out thousands of genes in a single experiment. Their bottleneck is interpretation. But not every cell type works with pooled approaches. Some labs, such as theLundberg Lab at Stanford, run smaller, focused screens, and their bottleneck comes earlier: deciding which genes to target in the first place. Because a single focused screen can cost upwards of $20,000 and costs increase with size, labs typically target a few hundred genes they think aremost likelyto be involved in a given condition. The conventional process involves a team of grad students and postdocs sitting around a Google spreadsheet, adding candidate genes one by one with a sentence of justification, or maybe a link to a paper. It's an educated guessing game, informed by literature reviews, expertise, and intuition, but constrained by human bandwidth. It’s also fallible, based as it is on what other scientists already figured out and written down, and what the humans in the room happen to recall. The Lundberg Lab is using Claude to flip that approach. Instead of asking “what guesses can we make based on what researchers have already studied?”, their system asks “whatshouldbe studied, based on molecular properties?” The team built a map of every known molecule in the cell—proteins, RNA, DNA—and how they relate to each other. They mapped out which proteins bind together, which genes code for which products, and which molecules are structurally similar. They can then give Claude a target—for instance which genes might govern a particular cellular structure or process—and Claude navigates that map to identify candidate genes based on their biological properties and relationships. The Lundberg lab is currently running an experiment to study how well this approach works. To do so, they needed to identify a topic where very little research had been done (if they’d looked at something well-studied, Claude might already know about the established findings). They chose primary cilia: antenna-like appendages on cells that we still know little about but which are implicated in a variety of developmental and neurological disorders. Next, they’ll run a whole genome screen to see which genes actually affect cilia formation, and establish the ground-truth. The test is to compare human experts to Claude. The humans will use the spreadsheet approach to make their guesses. Claude will generate its own using the molecular relationship map. If Claude catches (hypothetically) 150 out of 200, and the humans catch 80 out of 200, that's proof the approach works better. Even if they're about equal in discovering the genes, it’s still likely Claude works much faster, and could make the whole research process more efficient. If the approach works, the team envisions it becoming a standard first step in focused perturbation screening. Instead of gambling on intuition or using brute-force approaches that have become prevalent in contemporary research, labs could make informed bets about which genes to target—getting better results without needing the infrastructure for whole-genome screening. None of these systems are perfect. But they point to the ways that in just a few short years scientists have begun to incorporate AI as a research partner capable of far more than basic tasks—indeed, increasingly able to speed up, and in some cases even replace, many different aspects of the research process.  In speaking with these labs, a common theme emerged: the usefulness of the tools they’ve built continues to grow in concert with AI capabilities. Each model release brings noticeable improvements. Where just two years ago earlier models were limited to writing code or summarizing papers, more powerful agents have begun, if slowly, to replicate the very work those papers describe.  As tools advance and AI models continue to grow more intelligent, we’re continuing to watch and learn from how scientific discovery develops along with them.  For more detail on the expanded Claude for Life Sciences capabilities,see here, and ourtutorials here. We’re also continuing to acceptapplicationsto our AI for Science program. Applications will be reviewed by our team, including subject matter experts in relevant fields. </description>
      <author>Anthropic</author>
      <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/accelerating-scientific-research</guid>
    </item>
    <item>
      <title>Introducing Labs</title>
      <link>https://www.anthropic.com/news/introducing-anthropic-labs</link>
      <description>Our models are evolving at a rapid clip, and each new release brings another leap in capabilities. Building product experiences around these emerging capabilities requires different motions working in partnership: tinkering and experimenting at the edge of what Claude can do, testing unpolished versions with early users to find what works, and taking what lands and scaling it into products our customers can rely on.  This approach has produced Claude Code, which grew from a research preview to abillion-dollar productin six months; the Model Context Protocol (MCP) which, at100M monthly downloads, has become the industry standard for connecting AI to tools and data;Skills,Claude in Chrome, andCowork, which launched as a research preview yesterday to bring Claude’s agentic capabilities to desktop.  Today we’re building on this approach with the expansion of Labs, a team focused on incubating experimental products at the frontier of Claude’s capabilities. Mike Krieger—who co-founded Instagram and has spent the past two years as Anthropic’s Chief Product Officer—is joining Labs to build alongside Ben Mann. Ami Vora—who joined Anthropic at the end of 2025—will lead the Product organization, partnering closely with Rahul Patil, our CTO, to scale the Claude experiences that millions of users rely on every day.  “The speed of advancement in AI demands a different approach to how we build, how we organize, and where we focus. Labs gives us room to break the mold and explore,” said Daniela Amodei, President of Anthropic. “We now have the right structure in place to support the most critical motions for our product organization—discovering experimental products at the frontier of Claude’s capabilities, and scaling them responsibly to meet the needs of our enterprise customers and growing user base.” We're hiring builders with a track record of creating products people love and shaping emerging technology with care. If you’d like to build with us at the very frontier of AI capabilities,we want to hear from you.</description>
      <author>Anthropic</author>
      <pubDate>Tue, 13 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/introducing-anthropic-labs</guid>
    </item>
    <item>
      <title>Advancing Claude in healthcare and the life sciences</title>
      <link>https://www.anthropic.com/news/healthcare-life-sciences</link>
      <description>Introducing Claude for Healthcare with HIPAA-ready infrastructure, plus expanded Life Sciences tools for clinical trials and regulatory submissions. New connectors to CMS, Medidata, and ClinicalTrials.gov.</description>
      <author>Anthropic</author>
      <pubDate>Sun, 11 Jan 2026 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/healthcare-life-sciences</guid>
    </item>
    <item>
      <title>Sharing our compliance framework for California's Transparency in Frontier AI Act</title>
      <link>https://www.anthropic.com/news/compliance-framework-SB53</link>
      <description>On January 1, California's Transparency in Frontier AI Act (SB 53) will go into effect. It establishes the nation’s first frontier AI safety and transparency requirements for catastrophic risks.  While we have long advocated for a federal framework, AnthropicendorsedSB 53 because we believe frontier AI developers like ourselves should be transparent about how they assess and manage these risks. Importantly, the law balances the need for strong safety practices, incident reporting, and whistleblower protections—while preserving flexibility in how developers implement their safety measures, and exempting smaller companies from unnecessary regulatory burdens.  One of the law’s key requirements is that frontier AI developers publish a framework describing how they assess and manage catastrophic risks. Our Frontier Compliance Framework (FCF) is now available to the public,here. Below, we discuss what’s included within it, and highlight what we think should come next for frontier AI transparency.   Our FCF describes how we assess and mitigate cyber offense, chemical, biological, radiological, and nuclear threats, as well as the risks of AI sabotage and loss of control, for our frontier models. The framework also lays out our tiered system for evaluating model capabilities against these risk categories and explains our approach to mitigations. It also covers how we protect model weights and respond to safety incidents.  Much of what's in the FCF reflects an evolution of practices we've followed for years. Since 2023, ourResponsible Scaling Policy(RSP) has outlined our approach to managing extreme risks from advanced AI systems and informed our decisions about AI development and deployment. We also release detailed system cards when we launch new models, which describe capabilities, safety evaluations, and risk assessments. Other labs have voluntarily adopted similar approaches. Under the new law going into effect on January 1, those types of transparency practices are mandatory for those building the most powerful AI systems in California.  Moving forward, the FCF will serve as our compliance framework for SB 53 and other regulatory requirements. The RSP will remain our voluntary safety policy, reflecting what we believe best practices should be as the AI landscape evolves, even when that goes beyond or otherwise differs from current regulatory requirements.   The implementation of SB 53 is an important moment. By formalizing achievable transparency practices that responsible labs already voluntarily follow, the law ensures these commitments can't be abandoned quietly later once models get more capable, or as competition intensifies. Now, a federal AI transparency framework enshrining these practices is needed to ensure consistency across the country.  Earlier this year, we proposed aframeworkfor federal legislation. It emphasizes public visibility into safety practices, without trying to lock in specific technical approaches that may not make sense over time. The core tenets of our framework include:   As AI systems grow more powerful, the public deserves visibility into how they're being developed and what safeguards are in place. We look forward to working with Congress and the administration to develop a national transparency framework that ensures safety while preserving America’s AI leadership. </description>
      <author>Anthropic</author>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/compliance-framework-SB53</guid>
    </item>
  </channel>
</rss>
