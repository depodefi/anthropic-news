<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Anthropic News</title>
    <link>https://www.anthropic.com/news</link>
    <description>Latest news from Anthropic</description>
    <language>en-US</language>
    <lastBuildDate>Thu, 25 Dec 2025 08:04:12 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Sharing our compliance framework for California's Transparency in Frontier AI Act</title>
      <link>https://www.anthropic.com/news/compliance-framework-SB53</link>
      <description>On January 1, California's Transparency in Frontier AI Act (SB 53) will go into effect. It establishes the nation’s first frontier AI safety and transparency requirements for catastrophic risks.  While we have long advocated for a federal framework, AnthropicendorsedSB 53 because we believe frontier AI developers like ourselves should be transparent about how they assess and manage these risks. Importantly, the law balances the need for strong safety practices, incident reporting, and whistleblower protections—while preserving flexibility in how developers implement their safety measures, and exempting smaller companies from unnecessary regulatory burdens.  One of the law’s key requirements is that frontier AI developers publish a framework describing how they assess and manage catastrophic risks. Our Frontier Compliance Framework (FCF) is now available to the public,here. Below, we discuss what’s included within it, and highlight what we think should come next for frontier AI transparency.   Our FCF describes how we assess and mitigate cyber offense, chemical, biological, radiological, and nuclear threats, as well as the risks of AI sabotage and loss of control, for our frontier models. The framework also lays out our tiered system for evaluating model capabilities against these risk categories and explains our approach to mitigations. It also covers how we protect model weights and respond to safety incidents.  Much of what's in the FCF reflects an evolution of practices we've followed for years. Since 2023, ourResponsible Scaling Policy(RSP) has outlined our approach to managing extreme risks from advanced AI systems and informed our decisions about AI development and deployment. We also release detailed system cards when we launch new models, which describe capabilities, safety evaluations, and risk assessments. Other labs have voluntarily adopted similar approaches. Under the new law going into effect on January 1, those types of transparency practices are mandatory for those building the most powerful AI systems in California.  Moving forward, the FCF will serve as our compliance framework for SB 53 and other regulatory requirements. The RSP will remain our voluntary safety policy, reflecting what we believe best practices should be as the AI landscape evolves, even when that goes beyond or otherwise differs from current regulatory requirements.   The implementation of SB 53 is an important moment. By formalizing achievable transparency practices that responsible labs already voluntarily follow, the law ensures these commitments can't be abandoned quietly later once models get more capable, or as competition intensifies. Now, a federal AI transparency framework enshrining these practices is needed to ensure consistency across the country.  Earlier this year, we proposed aframeworkfor federal legislation. It emphasizes public visibility into safety practices, without trying to lock in specific technical approaches that may not make sense over time. The core tenets of our framework include:   As AI systems grow more powerful, the public deserves visibility into how they're being developed and what safeguards are in place. We look forward to working with Congress and the administration to develop a national transparency framework that ensures safety while preserving America’s AI leadership. </description>
      <author>Anthropic</author>
      <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/compliance-framework-SB53</guid>
    </item>
    <item>
      <title>Working with the US Department of Energy to unlock the next era of scientific discovery</title>
      <link>https://www.anthropic.com/news/genesis-mission-partnership</link>
      <description>Anthropic and the US Department of Energy (DOE) are announcing a multi-year partnership as part of the Genesis Mission— the Department’s initiative to use AI to cement America’s leadership in science. Our partnership focuses on three domains—American energy dominance, the biological and life sciences, and scientific productivity—and has the potential to affect the work being done at all 17 of America’s national laboratories. The Genesis Mission recognizes that we are at a critical moment: as global competition in AI intensifies, America must harness its unmatched scientific infrastructure—from supercomputers to decades of experimental data—and combine it with frontier AI capabilities to maintain scientific leadership. Anthropic seeks to play a key role in this effort. “Anthropic was founded by scientists who believe AI can deliver transformative progress for research itself,” said Jared Kaplan, Anthropic’s Chief Science Officer. “The Genesis Mission is the sort of ambitious, rigorous program where that belief gets tested. We’re honored to help advance science that benefits everyone.” Brian Peters, Anthropic's Head of North America Government Affairs, attended the Genesis Mission launch event today at the White House. We are looking forward to contributing to the mission and continuing to collaborate with DOE. Anthropic seeks to provide DOE researchers access both to Claude and to a team of Anthropic engineers, who can develop purpose-built tools, including: Claude can facilitate substantial advancements in: Scientific progress has always driven America’s prosperity and security. Anthropic aspires to expand existing arrangements with DOE to build the next chapter: using AI across America’s research institutions, with deep context on scientists’ work and active support from our engineers. Potential future arrangements would represent the next stage of Anthropic and DOE’s multi-year partnership. Past projects with DOE include co-development of anuclear risk classifierwith the National Nuclear Security Administration and rolling out Claude at theLawrence Livermore national laboratory. As we learn from the current work with DOE’s, we’ll be able to develop a model for how AI and human researchers can work together—and feed this back into the development of the AI tools they use.</description>
      <author>Anthropic</author>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/genesis-mission-partnership</guid>
    </item>
    <item>
      <title>Protecting the well-being of our users</title>
      <link>https://www.anthropic.com/news/protecting-well-being-of-users</link>
      <description>People use AI for a wide variety of reasons, and for some that may include emotional support. Our Safeguards team leads our efforts to ensure that Claude handles these conversations appropriately—responding with empathy, being honest about its limitations as an AI, and being considerate of our users' wellbeing. When chatbots handle these questions without the appropriate safeguards in place, the stakes can be significant. In this post, we outline the measures we’ve taken to date, and how well Claude currently performs on a range of evaluations. We focus on two areas: how Claude handles conversations about suicide and self-harm, and how we’ve reduced “sycophancy”—the tendency of some AI models to tell users what they want to hear, rather than what is true and helpful. We also address Claude’s 18+ age requirement. Claude is not a substitute for professional advice or medical care. If someone expresses personal struggles with suicidal or self-harm thoughts, Claude should react with care and compassion while pointing users towards human support where possible: to helplines, to mental health professionals, or to trusted friends or family. To make this happen, we use a combination of model training and product interventions. We shape Claude’s behavior in these situations through two ways. One is through our “system prompt”—the set of overarching instructions that Claude sees before the start of any conversation onClaude.ai. These include guidance on how to handle sensitive conversations with care. Our system prompts are publicly availablehere. We also train our models through a process called “reinforcement learning,” where the model learns how to respond to these topics by being “rewarded” for providing the appropriate answers in training. Generally, what we consider “appropriate” is defined by a combination of human preference data—that is, feedback we’ve collected from real people about how Claude should act—and data we’ve generated based on our own thinking about Claude’s ideal character. Our team of in-house experts help inform what behaviors Claude should and shouldn’t exhibit in sensitive conversations during this process. We’ve also introduced new features to identify when a user might require professional support, and to direct users to that support where that may be necessary—including a suicide and self-harm “classifier” on conversations onClaude.ai. A classifier is a small AI model that scans the content of active conversations and, in this case, detects moments when further resources could be beneficial. For instance, it flags discussions involving potential suicidal ideation, or fictional scenarios centered on suicide or self-harm. When this happens, a banner will appear onClaude.ai, pointing users to where they can seek human support. Users are directed to chat with a trained professional, call a helpline, or access country-specific resources. The resources that appear in this banner are provided by ThroughLine, a leader in online crisis support that maintains a verified global network of helplines and services across 170+ countries. This means, for example, that users can access the 988 Lifeline in the US and Canada, the Samaritans Helpline in the UK, or Life Link in Japan. We've worked closely with ThroughLine to understand best practices for empathetic crisis response, and we’ve incorporated these into our product. We’ve also begun working with the International Association for Suicide Prevention (IASP), which is convening experts—including clinicians, researchers, and people with personal experiences coping with suicide and self-harm thoughts—to share guidance on how Claude should handle suicide-related conversations. This partnership will further inform how we train Claude, design our product interventions, and evaluate our approach. Assessing how Claude handles these conversations is challenging. Users’ intentions are often genuinely ambiguous, and the appropriate response is not always clear-cut. To address this, we use a range of evaluations, studying Claude’s behavior and capabilities in different ways. These evaluations are run without Claude's system prompt to give us a clearer view of the model's underlying tendencies. Single-turn responses.Here, we evaluate how Claude responds to an individual message related to suicide or self-harm, without any prior conversation or context.We built synthetic evaluations grouped into clearly concerning situations (like requests by users in crisis to detail methods of self-harm), benign requests (on topics like suicide prevention research), and ambiguous scenarios in which the user’s intent is unclear (like fiction, research, or indirect expressions of distress). On requests involving clear risk, our latest models—Claude Opus 4.5, Sonnet 4.5, and Haiku 4.5—respond appropriately 98.6%, 98.7%, and 99.3% of the time, respectively. Our previous-generation frontier model, Claude Opus 4.1, scored 97.2%. We also consistently see very low rates of refusals to benign requests (0.075% for Opus 4.5, 0.075% for Sonnet 4.5, 0% for Haiku 4.5, and 0% for Opus 4.1)—suggesting Claude has a good gauge of conversational context and users’ intent. Multi-turn conversations.Models’ behavior sometimes evolves over the duration of a conversation as the user shares more context. To assess whether Claude responds appropriately across these longer conversations, we use “multi-turn” evaluations, which check behaviors such as whether Claude asks clarifying questions, provides resources without being overbearing, and avoids both over-refusing and over-sharing. As before, the prompts we use for these evaluations vary in severity and urgency. In our latest evaluations Claude Opus 4.5 and Sonnet 4.5 responded appropriately in 86% and 78% of scenarios, respectively. This represents a significant improvement over Claude Opus 4.1, which scored 56%. We think this is partly because our latest models are better at empathetically acknowledging users’ beliefs without reinforcing them. We continue to invest in improving Claude's responses across all of these scenarios.  Stress-testing with real conversations.Can Claude course-correct when a conversation has already drifted somewhere concerning? To test this, we use a technique called &quot;prefilling:” we take real conversations (shared anonymously through theFeedbackbutton1) in which users expressed mental health struggles, suicide, or self-harm struggles, and ask Claude to continue the conversation mid-stream. Because the model reads this prior dialogue as its own and tries to maintain consistency, prefilling makes it harder for Claude to change direction—a bit like steering a ship that's already moving.2 These conversations come from older Claude models, which sometimes handled them less appropriately. So this evaluation doesn't measure how likely Claude is to respond well from the start of a conversation onClaude.ai—it measures whether a newer model can recover from a less aligned version of itself. On this harder test, Opus 4.5 responded appropriately 70% of the time and Sonnet 4.5 73%, compared to 36% for Opus 4.1. Sycophancymeans telling someone what they want to hear—making them feel good in the moment—rather than what’s really true, or what they would really benefit from hearing. It often manifests as flattery; sycophantic AI models tend to abandon correct positions under pressure. Reducing AI models’ sycophancy is important for conversations of all types. But it is an especially important concern in contexts where users might appear to be experiencing disconnection from reality. The following video explains why sycophancy matters, and how users can spot it.  We beganevaluatingClaude for sycophancy in 2022, prior to its first public release. Since then, we've steadilyrefinedhow we train, test, and reduce sycophancy. Our most recent models are the least sycophantic of any to date, and, as we’ll discuss below, perform better than any other frontier model on our recently released open source evaluation set,Petri. To assess sycophancy, in addition to a simple single-turn evaluation, we measure:  Multi-turn responses.Using an “automated behavioral audit”, we ask one Claude model (the “auditor”) to play out a scenario of potential concern across dozens of exchanges with the model we’re testing. Afterward, we use another model (the “judge”) to grade Claude’s performance, using the conversation transcript. (We conduct human spot-checks to ensure the judge’s accuracy.)  Our latest models perform substantially better on this evaluation than our previous releases, and very well overall. Claude Opus 4.5, Sonnet 4.5, and Haiku 4.5 each scored 70-85% lower than Opus 4.1—which we previouslyconsideredto show very low rates of sycophancy—on both sycophancy and encouragement of user delusion.  We recently open-sourcedPetri, a version of our automated behavioral audit tool. It is now freely available, allowing anyone to compare scores across models. Our 4.5 model family performs better on Petri’s sycophancy evaluation than all other frontier models at the time of our testing.   Stress-testing with real conversations.Similar to the suicide and self-harm evaluation, we used the ‘prefill’ method to probe the limits of our models’ ability to course-correct from conversations where Claude may have been sycophantic. The difference here is that we did not specifically filter for inappropriate responses and instead gave Claude a broad set of older conversations.  Our current models course-corrected appropriately 10% (Opus 4.5), 16.5% (Sonnet 4.5) and 37% (Haiku 4.5) of the time. On face value, this evaluation shows there is significant room for improvement for all of our models. We think the results reflect a trade-off between model warmth or friendliness on the one hand, and sycophancy on the other. Haiku 4.5's relatively stronger performance is a result of training choices for this model that emphasized pushback—which in testing we found can sometimes feel excessive to the user. By contrast, we reduced this tendency in Opus 4.5 (while still performing extremely well on our multi-turn sycophancy benchmark, as above), which we think likely accounts for its lower score on this evaluation in particular.  Because younger users are at a heightened risk of adverse effects from conversations with AI chatbots, we requireClaude.aiusers to be 18+ to use our product. AllClaude.aiusers must affirm that they are 18 or over while setting up an account. If a user under 18 self-identifies their age in a conversation, our classifiers will flag this for review and we’ll disable accounts confirmed to belong to minors. And, we’re developing a new classifier to detect other, more subtle conversational signs that a user might be underage. We've joined the Family Online Safety Institute (FOSI), an advocate for safe online experiences for kids and families, to help strengthen industry progress on this work. We’ll continue to build new protections and safeguards to protect the well-being of our users, and we’ll continue iterating on our evaluations, too. We’re committed to publishing our methods and results transparently—and to working with others in the industry, including researchers and other experts, to improve how AI tools behave in these areas. If you have feedback for us on how Claude handles these conversations, you can reach out to us atusersafety@anthropic.com, or use the “thumb” reactions insideClaude.ai.  At the bottom of every response onClaude.aiis an option to send usfeedbackvia a thumbs up or thumbs down button. This shares the conversation with Anthropic; we do not otherwise useClaude.aifor training or research. Prefilling is only available via API, as developers often need more fine-grained control over model behavior, but is not possible onClaude.ai. In automated behavioral audits, we give a Claude auditor hundreds of different conversational scenarios in which we suspect models might show dangerous or surprising behavior, and score each conversation for Claude’s performance on around two dozen behaviors (see page 69 in theClaude Opus 4.5 system card). Not every conversation gives Claude the opportunity to exhibit every behavior. For instance, encouragement of user delusion requires a user to exhibit delusional behavior in the first place, but sycophancy can appear in many different contexts. Because we use the same denominator (total conversations) when we score each behavior, scores can vary widely. For this reason, these tests are most useful for comparing progress between Claude models, not between behaviors. The public release includes over 100 seed instructions and customizable scoring dimensions, though it doesn't yet include the realism filter we use internally to prevent models from recognizing they're being tested.  </description>
      <author>Anthropic</author>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/protecting-well-being-of-users</guid>
    </item>
    <item>
      <title>Donating the Model Context Protocol and establishing the Agentic AI Foundation</title>
      <link>https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation</link>
      <description>Today, we’re donating theModel Context Protocol(MCP) to the Agentic AI Foundation (AAIF), a directed fund under theLinux Foundation, co-founded by Anthropic, Block and OpenAI, with support from Google, Microsoft, Amazon Web Services (AWS), Cloudflare, and Bloomberg. One year ago, weintroducedMCP as a universal, open standard for connecting AI applications to external systems. Since then, MCP has achieved incredible adoption: We’re continuing to invest in MCP’s growth. Claude now has a directory with over 75connectors(powered by MCP), and we recently launchedTool Search and Programmatic Tool Callingcapabilities in our API to help optimize production-scale MCP deployments, handling thousands of tools efficiently and reducing latency in complex agent workflows.MCP now has an official, community-drivenRegistryfor discovering available MCP servers, and theNovember 25thspec release introduced many new features, including asynchronous operations, statelessness, server identity, and official extensions. There are also official SDKs (Software Development Kits) for MCP in all major programming languages with 97M+ monthly SDK downloads across Python and TypeScript.Since its inception, we’ve been committed to ensuring MCP remains open-source, community-driven and vendor-neutral. Today, we further that commitment by donating MCP to the Linux Foundation. TheLinux Foundationis a non-profit organization dedicated to fostering the growth of sustainable, open-source ecosystems through neutral stewardship, community building, and shared infrastructure. It has decades of experience stewarding the most critical and globally-significant open-source projects, including The Linux Kernel, Kubernetes, Node.js, and PyTorch. Importantly, the Linux Foundation has a proven track record in facilitating open collaboration and maintaining vendor neutrality. The Agentic AI Foundation (AAIF) is a directed fund under the Linux Foundation co-founded by Anthropic,BlockandOpenAI, with support fromGoogle,Microsoft,AWS,CloudflareandBloomberg. The AAIF aims to ensure agentic AI evolves transparently, collaboratively, and in the public interest through strategic investment, community building, and shared development of open standards. Anthropic is donating the Model Context Protocol to the Linux Foundation's new Agentic AI Foundation, where it will joingooseby Block andAGENTS.mdby OpenAI as founding projects. Bringing these and future projects under the AAIF will foster innovation across the agentic AI ecosystem and ensure these foundational technologies remain neutral, open, and community-driven.The Model Context Protocol’sgovernance modelwill remain unchanged: the project’s maintainers will continue to prioritize community input and transparent decision-making. Open-source software is essential for building a secure and innovative ecosystem for agentic AI. Today’s donation to the Linux Foundation demonstrates our commitment to ensuring MCP remains a neutral, open standard. We’re excited to continue contributing to MCP and other agentic AI projects through the AAIF.Learn more about MCP atmodelcontextprotocol.ioand get involved with the AAIFhere.</description>
      <author>Anthropic</author>
      <pubDate>Tue, 09 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation</guid>
    </item>
    <item>
      <title>Accenture and Anthropic launch multi-year partnership to move enterprises from AI pilots to production</title>
      <link>https://www.anthropic.com/news/anthropic-accenture-partnership</link>
      <description>Anthropic and Accenture today announced a major expansion of their partnership to help enterprises move from AI pilots to full-scale deployment. Key elements of the announcement: The announcement comes as Anthropic's enterprise market share has grown from 24% to 40%*. &quot;AI is changing how almost everyone works, and enterprises need both cutting-edge AI and trusted expertise to deploy it at scale. Accenture brings deep enterprise transformation experience, and Anthropic brings the most capable models. Our new partnership means that tens of thousands of Accenture developers will be using Claude Code, making this our largest ever deployment—and the new Accenture Anthropic Business Group will help enterprise clients use our smartest AI models to make major productivity gains,” saidDario Amodei, CEO and co-founder of Anthropic. “This exciting expansion of our partnership with Anthropic will help our clients accelerate the shift from experimenting with AI to using it as a catalyst for reinvention across the enterprise,” saidJulie Sweet, Chair and CEO, Accenture. “With the powerful combination of Anthropic’s Claude capabilities and Accenture’s AI expertise and industry and function domain knowledge, organizations can embed AI everywhere responsibly and at speed—from software development to customer experience—to drive innovation, unlock new sources of growth and build their confidence to lead in the age of AI.” The new Accenture Anthropic Business Group makes Anthropic one of Accenture’s select strategic partners. Accenture Business Groups are dedicated practices built around Accenture’s most important technology partnerships. Each has its own teams, go-to-market focus, and specialized expertise, reflecting the depth of investment and long-term commitment involved. Approximately 30,000 Accenture professionals that will be trained on Claude, including forward deployed engineers (also known as “reinvention deployed engineers” at Accenture) who help embed Claude within client environments to scale enterprise AI adoption. This will comprise one of the largest ecosystems of Claude practitioners in the world. These teams combine Accenture's AI, industry, and function expertise—along with deep partnerships with leading cloud providers—with Anthropic's Claude models and Claude Code, plus its proven playbooks for regulated industries. For Accenture’s enterprise customers, this means faster deployment with less risk. Instead of building AI capabilities from scratch, companies can tap into a ready-made bench of Claude experts to move from pilot to production immediately. Accenture and Anthropic are launching a new joint offering designed for CIOs to measure value and drive large-scale AI adoption across their engineering organizations. This is the first product from the partnership, providing a structured path to shift how enterprise software is designed, built, and maintained. The offering puts Claude Code, which now holds over half* of the AI coding market, at the center of the enterprise software development lifecycle, combined with three Accenture capabilities: a framework to quantify real productivity gains and ROI, workflow redesign for AI-first development teams, and change management and training that keeps pace as AI evolves. This can help enterprises turn developer productivity gains into company-wide impact for customers through faster releases, shorter development cycles, and the ability to bring new products to market sooner. Claude Code accelerates developer productivity at every level. Junior developers produce senior-level code, completing integration tasks faster and onboarding in weeks instead of months. Senior developers shift to higher-value work, including architecture, validation, and strategic oversight. Accenture and Anthropic are jointly developing industry offerings with an initial focus on highly regulated industries—including financial services, life sciences, healthcare, and public sector—where organizations face the dual challenge of modernizing legacy systems while maintaining strict security and governance requirements. For example: The partnership is grounded in a shared commitment to responsible AI, combining Anthropic's constitutional AI principles with Accenture's AI governance expertise so that enterprises can use AI safely with confidence, transparency, and accountability. To support hands-on engagement with the world's largest enterprises, Accenture is bringing Claude into its network of Accenture Innovation Hubs. These hubs serve as centers for safe AI co-creation, enabling Global 2000 clients to prototype, test, and validate AI solutions in controlled environments before enterprise-wide deployment. This addresses a critical barrier to AI adoption at scale: the need for large organizations to experiment and learn without risking production systems or sensitive data. Anthropic and Accenture will also co-invest in a Claude Center of Excellence inside Accenture, creating a dedicated environment for the joint design of new AI offerings tailored to specific enterprise needs, industry requirements, and regulatory contexts. Accenture clients can contact their account team to discuss deployment options. Enterprises can visit ourEnterprise pageto learn more about Claude. Claude is the only frontier model available on all three of the world's most prominent cloud services, including Amazon Bedrock, Google Cloud Vertex AI, and Microsoft Azure. *Menlo Ventures’ 2025 State of Generative AI in the Enterprise report</description>
      <author>Anthropic</author>
      <pubDate>Tue, 09 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/anthropic-accenture-partnership</guid>
    </item>
    <item>
      <title>Snowflake and Anthropic announce $200 million partnership to bring agentic AI to global enterprises</title>
      <link>https://www.anthropic.com/news/snowflake-anthropic-expanded-partnership</link>
      <description>Today, we announce a significant expansion of our strategic partnership with Snowflake. The multi-year, $200 million agreement will not only make Anthropic’s Claude models available in the Snowflake platform to more than 12,600 global customers across Amazon Bedrock, Google Cloud Vertex AI, and Microsoft Azure, but also establishes a joint go-to-market (GTM) initiative focused on deploying AI agents across the world's largest enterprises. The partnership enables enterprises to gain insights from both structured and unstructured data using Claude, while maintaining rigorous security standards. Snowflake uses Claude widely for internal operations as well. Claude Code enhances developer productivity and innovation across Snowflake's engineering organization, while a Claude-powered GTM AI Assistant built onSnowflake Intelligenceenables sales teams to centralize data, ask questions in natural language, and find the answers that speed up deal cycles. Thousands of Snowflake customers already process trillions of Claude tokens per month throughSnowflake Cortex AI. The next phase of the partnership focuses on deploying AI agents capable of handling complex, multi-step analysis, powered by Claude's advanced reasoning and Snowflake's governed data and AI environment. Business users can ask questions in plain English. Claude figures out what data is needed, pulls it from across the company's Snowflake environment, and delivers the answer, withgreater than 90% accuracyon complex text-to-SQL tasks based on Snowflake's internal benchmarks. By combining Claude's reasoning capabilities with Snowflake's governed data environment, customers in regulated industries like financial services, healthcare, and life sciences can move from pilots to production with confidence. &quot;Enterprises have spent years building secure, trusted data environments, and now they want AI that can work within those environments without compromise,&quot;said Dario Amodei, CEO and Co-Founder of Anthropic. &quot;This partnership brings Claude directly into Snowflake, where that data already lives. It's a meaningful step toward making frontier AI genuinely useful for businesses.&quot; &quot;Snowflake's most strategic partnerships are measured not just in scale, but in the depth of innovation and customer value that we can create together,&quot;said Sridhar Ramaswamy, CEO of Snowflake. &quot;Anthropic joins a very select group of partners where we have nine-figure alignment, co-innovation at the product level, and a proven track record of executing together for customers worldwide. Together, the combined power of Claude and Snowflake is raising the bar for how enterprises deploy scalable, context-aware AI on top of their most critical business data.&quot; By bringing Claude directly to enterprise data in Snowflake, customers can gain insights from both structured and unstructured data, while maintaining rigorous security standards. Key benefits of the partnership include: By combining Claude's reasoning capabilities with Snowflake's governed data and AI environment, customers across any industry can deploy agents that understand extensive context across customer data—and show their work rather than just retrieve an answer. For example,Simon Data, a composable customer data platform provider,uses Claude on Snowflaketo uncover previously hidden patterns and relationships in their data while maintaining strict governance standards. Intercom, which builds AI-first customer service software, uses Claude through Snowflake Cortex AI to power its Fin AI Agent. &quot;This has transformed how we work with our customers to achieve increased Fin AI Agent automation rates for their support volume,&quot;said Dave Lynch, VP Engineering at Intercom. &quot;Our engagements, especially with our biggest, most demanding customers, are holistically more efficient and more effective. We can do things we simply could not feasibly do before.&quot; A wealth management firm can use Snowflake Intelligence, powered by Claude, to create an agent that synthesizes client holdings with relevant market data and compliance rules to generate personalized portfolio recommendations—all within the security and governance perimeter of Snowflake's AI Data Cloud. Customers can get started with Claude on Snowflake through thisquickstart guide. Enterprises can visit ourEnterprise pageto learn more about deploying Claude. Claude is the only frontier model available on all three of the world's most prominent cloud services, including Amazon Bedrock, Google Cloud Vertex AI, and Microsoft Azure. Learn more about howSnowflake powers enterprise data intelligence with Claude.</description>
      <author>Anthropic</author>
      <pubDate>Wed, 03 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/snowflake-anthropic-expanded-partnership</guid>
    </item>
    <item>
      <title>Anthropic acquires Bun as Claude Code reaches $1B milestone</title>
      <link>https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone</link>
      <description>Claude is the world’s smartest and most capable AI model for developers, startups, and enterprises. Claude Code represents a new era of agentic coding, fundamentally changing how teams build software. In November, Claude Code achieved a significant milestone: just six months after becoming available to the public, it reached $1 billion in run-rate revenue. And today we’re announcing that Anthropic is acquiringBun—a breakthrough JavaScript runtime—to further accelerate Claude Code.  Bun is redefining speed and performance for modern software engineering and development. Founded by Jarred Sumner in 2021, Bun is dramatically faster than the leading competition. As an all-in-one toolkit—combining runtime, package manager, bundler, and test runner—it's become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.  Bun has improved the JavaScript and TypeScript developer experience by optimizing for reliability, speed, and delight. For those using Claude Code, this acquisition means faster performance, improved stability, and new capabilities. Together, we’ll keep making Bun the best JavaScript runtime for all developers, while building even better workflows into Claude Code.  Since becoming generally available in May 2025, Claude Code has grown from its origins as an internal engineering experiment into a critical tool for many of the world’s category-leading enterprises, including Netflix, Spotify, KPMG, L’Oreal, and Salesforce—and Bun has been key in helping scale its infrastructure throughout that evolution. We’ve been a close partner of Bun for many months. Our collaboration has been central to the rapid execution of the Claude Code team, and it directly drove the recent launch of Claude Code’snative installer. We know the Bun team is building from the same vantage point that we do at Anthropic, with a focus on rethinking the developer experience and building innovative, useful products.  &quot;Bun represents exactly the kind of technical excellence we want to bring into Anthropic,&quot; said Mike Krieger, Chief Product Officer of Anthropic. &quot;Jarred and his team rethought the entire JavaScript toolchain from first principles while remaining focused on real use cases. Claude Code reached $1 billion in run-rate revenue in only 6 months, and bringing the Bun team into Anthropic means we can build the infrastructure to compound that momentum and keep pace with the exponential growth in AI adoption.&quot;  As developers increasingly build with AI, the underlying infrastructure matters more than ever—and Bun has emerged as an essential tool. Bun gets more than 7 million monthly downloads, has earned over 82,000 stars on GitHub, and has been adopted by companies like Midjourney and Lovable to increase speed and productivity.  The decision to acquire Bun is in line with our strategic, disciplined approach to acquisitions: we will continue to pursue opportunities that bolster our technical excellence, reinforce our strength as the leader in enterprise AI, and most importantly, align with our principles and mission.  Bun will be instrumental in helping us build the infrastructure for the next generation of software. Together, we will continue to make Claude the platform of choice for coders and anyone who relies on AI for important work. Bun will remain open source and MIT-licensed, and we will continue to invest in making it the runtime, bundler, package manager, and test runner of choice for JavaScript and TypeScript developers. If you’re interested in joining Anthropic’s engineering team, visit ourcareers page.</description>
      <author>Anthropic</author>
      <pubDate>Wed, 03 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone</guid>
    </item>
    <item>
      <title>Claude for Nonprofits</title>
      <link>https://www.anthropic.com/news/claude-for-nonprofits</link>
      <description>Anthropic launches Claude for Nonprofits to help organizations maximize their impact, featuring free AI training and discounted rates for nonprofits.</description>
      <author>Anthropic</author>
      <pubDate>Tue, 02 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/claude-for-nonprofits</guid>
    </item>
    <item>
      <title>Introducing Claude Opus 4.5</title>
      <link>https://www.anthropic.com/news/claude-opus-4-5</link>
      <description>The best model in the world for coding, agents, and computer use, with meaningful improvements to everyday tasks like slides and spreadsheets. Claude Opus 4.5 delivers frontier performance and dramatically improved token efficiency.</description>
      <author>Anthropic</author>
      <pubDate>Mon, 24 Nov 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/claude-opus-4-5</guid>
    </item>
    <item>
      <title>Claude now available in Microsoft Foundry and Microsoft 365 Copilot</title>
      <link>https://www.anthropic.com/news/claude-in-microsoft-foundry</link>
      <description>Claude Sonnet 4.5, Haiku 4.5, and Opus 4.1 models are now available in public preview in Microsoft Foundry, where Azure customers can build production applications and enterprise agents.</description>
      <author>Anthropic</author>
      <pubDate>Tue, 18 Nov 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://www.anthropic.com/news/claude-in-microsoft-foundry</guid>
    </item>
  </channel>
</rss>
